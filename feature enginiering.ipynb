{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71cced3-3598-47d9-97f7-872758f58cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1.\n",
    "\n",
    "Missing values in a dataset refer to the absence of data or information for one or more variables in certain observations or records. They can occur due to various reasons, such as data entry errors, equipment failures, survey non-responses, or intentional omissions.\n",
    "\n",
    "Handling missing values is essential for several reasons:\n",
    "\n",
    "Accurate analysis: Missing values can introduce bias and lead to incorrect or misleading conclusions if not handled properly. It is crucial to address missing values to ensure accurate data analysis and interpretation.\n",
    "\n",
    "Reliable models: Many machine learning algorithms cannot directly handle missing values. If missing values are not addressed, it can result in errors or suboptimal performance of the models. Handling missing values appropriately is necessary to build reliable and robust models.\n",
    "\n",
    "Data integrity: Missing values can affect the integrity of the dataset. By handling missing values properly, the dataset's integrity is maintained, and the quality of the data is preserved.\n",
    "\n",
    "Statistical power: Missing values reduce the effective sample size, which can impact the statistical power of the analysis. Handling missing values allows for the maximum utilization of available data, leading to more robust statistical analyses.\n",
    "\n",
    "Algorithms that are not affected by missing values or have built-in mechanisms to handle them include:\n",
    "\n",
    "Decision trees: Decision tree algorithms, such as C4.5 and CART, can handle missing values by creating surrogate splits or assigning default values based on available information.\n",
    "\n",
    "Random Forests: Random Forests are an ensemble of decision trees and can handle missing values by averaging predictions from multiple trees that consider different subsets of features.\n",
    "\n",
    "Gradient Boosting methods: Gradient Boosting algorithms, like XGBoost and LightGBM, can internally handle missing values by following a similar mechanism as decision trees.\n",
    "\n",
    "Naive Bayes: Naive Bayes classifiers can handle missing values by ignoring the missing attributes during probability calculations. The missing attributes are simply not considered in the model.\n",
    "\n",
    "k-Nearest Neighbors (k-NN): k-NN algorithms can handle missing values by imputing missing attribute values based on the values of the nearest neighbors.\n",
    "\n",
    "Support Vector Machines (SVM): SVM algorithms can handle missing values by omitting the instances with missing values during the training process and building the model based on the available data.\n",
    "\n",
    "It's worth noting that although these algorithms can handle missing values, the choice of handling strategy (e.g., imputation, deletion, or other methods) can still impact their performance and the validity of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee308c1f-9cb6-437a-9076-2a85af2967aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.\n",
    "There are several techniques used to handle missing data. Here are five commonly used techniques along with an example of each in Python:\n",
    "\n",
    "Deletion: In this technique, the rows or columns with missing values are removed from the dataset. This approach is suitable when the missing data is assumed to be missing completely at random (MCAR). However, it can lead to loss of information if the missing data is not MCAR.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [None, 6, 7, None, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dropping rows with missing values\n",
    "df_dropped = df.dropna()\n",
    "print(df_dropped)\n",
    "\n",
    "Mean/Median/Mode Imputation: Missing values are replaced with the mean, median, or mode of the respective variable. This technique is suitable when the missing data is assumed to be missing at random (MAR).\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [None, 6, 7, None, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_imputed)\n",
    "\n",
    "\n",
    "Forward/Backward Fill: Missing values are filled using the previous (forward fill) or next (backward fill) valid observation. This technique is useful when the missing data has a temporal or sequential pattern.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, None, 3, None, 5],\n",
    "        'B': [None, 6, None, 8, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Forward fill missing values\n",
    "df_forward = df.ffill()\n",
    "print(df_forward)\n",
    "\n",
    "Interpolation: Missing values are filled using interpolation techniques such as linear interpolation, polynomial interpolation, or spline interpolation. This technique is suitable when the missing data does not have a temporal pattern and can be estimated based on other variables.\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, None, 3, None, 5],\n",
    "        'B': [None, 6, None, 8, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Linear interpolation to fill missing values\n",
    "df_interpolated = df.interpolate(method='linear')\n",
    "print(df_interpolated)\n",
    "\n",
    "\n",
    "Model-based Imputation: A model is created to predict missing values based on other variables. Common techniques include regression-based imputation, k-nearest neighbors imputation, or matrix completion methods. This technique is suitable when the missing data has complex relationships with other variables.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "data = {'A': [1, 2, None, 4, 5],\n",
    "        'B': [None, 6, 7, None, 9]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Impute missing values using k-nearest neighbors\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print(df_imputed)\n",
    "\n",
    "These are just a few examples of techniques to handle missing data, and the choice of technique depends on the nature of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338cf4c4-6684-4b1b-b239-550e22bdf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3.\n",
    "Imbalanced data refers to a situation where the distribution of classes or categories in a dataset is highly skewed. In other words, one class has a significantly larger number of instances compared to the other class(es). For example, in a binary classification problem, if 95% of the instances belong to class A and only 5% belong to class B, it represents an imbalanced dataset.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several issues:\n",
    "\n",
    "Biased model performance: Most machine learning algorithms are designed to maximize overall accuracy, assuming a balanced distribution of classes. When faced with imbalanced data, these algorithms tend to be biased towards the majority class and achieve high accuracy by simply predicting the majority class for most instances. As a result, the minority class is often poorly predicted, leading to low sensitivity/recall for the minority class.\n",
    "\n",
    "Poor generalization: Imbalanced data can hinder the model's ability to generalize well to new, unseen data. The model may learn to overfit the majority class and fail to capture the patterns and characteristics of the minority class. This can lead to poor performance on real-world data, especially when the minority class is of particular interest or importance.\n",
    "\n",
    "Misleading evaluation metrics: Traditional evaluation metrics such as accuracy can be misleading when dealing with imbalanced data. A model that predicts the majority class for all instances may still achieve high accuracy, but it fails to identify the minority class instances correctly. Evaluation metrics like precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC) provide a more comprehensive assessment of model performance on imbalanced data.\n",
    "\n",
    "Decision-making bias: When deployed in real-world scenarios, models trained on imbalanced data can result in biased decision-making. For example, in a fraud detection system, if the model has not been properly trained on imbalanced data, it may fail to detect instances of fraud accurately, leading to financial losses.\n",
    "\n",
    "To address the issues caused by imbalanced data, various techniques can be applied, such as:\n",
    "\n",
    "Resampling methods: Oversampling the minority class (e.g., Random Oversampling, SMOTE) or undersampling the majority class (e.g., Random Undersampling, NearMiss) to rebalance the class distribution.\n",
    "Generating synthetic samples: Creating synthetic instances of the minority class using techniques like Synthetic Minority Over-sampling Technique (SMOTE).\n",
    "Cost-sensitive learning: Assigning different misclassification costs to different classes during model training to emphasize the importance of correctly predicting the minority class.\n",
    "Ensemble methods: Utilizing ensemble techniques like Bagging, Boosting, or stacking to combine multiple models and improve performance on the minority class.\n",
    "By addressing the imbalanced nature of the data, these techniques help in building models that are more accurate, have better generalization capabilities, and make informed decisions across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea6012-8940-4aa0-8abb-16c18ea60e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4.\n",
    "\n",
    "Up-sampling and down-sampling are techniques used to address the issue of imbalanced data by adjusting the class distribution in a dataset.\n",
    "\n",
    "Up-sampling: Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This can be achieved by duplicating existing instances of the minority class or generating synthetic samples based on the existing ones.\n",
    "Example of when up-sampling is required:\n",
    "Suppose we have a dataset for credit card fraud detection, where the majority class represents legitimate transactions (non-fraudulent) and the minority class represents fraudulent transactions. If the dataset has a significantly higher number of legitimate transactions compared to fraudulent transactions, it creates an imbalanced dataset. In this case, up-sampling the minority class by duplicating or generating synthetic fraudulent transactions can help balance the class distribution and provide the model with more examples to learn from, improving its ability to detect fraudulent transactions accurately.\n",
    "\n",
    "Down-sampling: Down-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. This can be achieved by randomly removing instances from the majority class or selecting a subset of instances that best represents the majority class.\n",
    "Example of when down-sampling is required:\n",
    "Suppose we have a dataset for medical diagnosis, where the majority class represents non-cancerous cases and the minority class represents cancer cases. If the dataset has a significantly higher number of non-cancerous cases compared to cancer cases, it creates an imbalanced dataset. In this case, down-sampling the majority class by randomly removing instances or selecting a representative subset of non-cancerous cases can help balance the class distribution. This ensures that the model receives an equal number of cancer and non-cancer cases during training, improving its ability to accurately predict cancer cases.\n",
    "\n",
    "Both up-sampling and down-sampling techniques aim to mitigate the impact of imbalanced data on model training. The choice between these techniques depends on the specific dataset, the imbalance severity, and the nature of the problem. It's important to carefully consider the potential consequences of each technique, as up-sampling may lead to overfitting or introducing synthetic noise, while down-sampling may result in information loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a464fb2-111c-4119-84c3-5fe69c88d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5.\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by applying various transformations or modifications to the existing data. It is commonly used in machine learning and deep learning tasks, especially when the available data is limited. Data augmentation helps in improving model performance, generalization, and robustness by introducing additional variations and diversity into the training data.\n",
    "\n",
    "One popular data augmentation technique is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE is specifically designed to address the class imbalance problem by generating synthetic samples for the minority class.\n",
    "\n",
    "Here's an explanation of how SMOTE works:\n",
    "\n",
    "Selecting a minority instance: SMOTE randomly selects an instance from the minority class.\n",
    "\n",
    "Finding its k nearest neighbors: SMOTE calculates the k nearest neighbors of the selected instance, typically using distance metrics like Euclidean distance.\n",
    "\n",
    "Generating synthetic samples: For each selected instance, SMOTE creates synthetic samples by interpolating between the selected instance and its k nearest neighbors. It randomly selects a neighbor and calculates the difference between the feature values of the selected instance and the neighbor. It then multiplies this difference by a random number between 0 and 1 and adds it to the selected instance's feature values. This process generates new synthetic instances within the feature space.\n",
    "\n",
    "Repeating the process: The process of selecting instances, finding nearest neighbors, and generating synthetic samples is repeated until the desired level of class balance or a specific number of synthetic samples is achieved.\n",
    "\n",
    "SMOTE effectively creates synthetic samples in the feature space, which helps in expanding the minority class and balancing the class distribution. By introducing new synthetic instances, SMOTE allows the model to learn from a more diverse range of examples, reducing the bias towards the majority class and improving the model's ability to correctly classify minority class instances.\n",
    "\n",
    "SMOTE can be implemented using various libraries such as imbalanced-learn in Python. Here's an example using the imbalanced-learn library:\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Load the imbalanced dataset\n",
    "data = pd.read_csv('imbalanced_dataset.csv')\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Apply SMOTE for oversampling the minority class\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# The resampled dataset now has balanced class distribution\n",
    "\n",
    "By applying SMOTE, the minority class is oversampled, resulting in a balanced dataset that can be used for training machine learning models, improving their performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ebce2-784b-4f85-b294-2ff353f2e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q6.\n",
    "\n",
    "In statistics and data analysis, outliers refer to data points that significantly deviate from the rest of the observations in a dataset. These data points are typically located far away from the central tendency of the data, such as the mean or median. Outliers can occur due to various reasons, including measurement errors, experimental errors, or genuine unusual observations.\n",
    "\n",
    "Handling outliers is essential for several reasons:\n",
    "\n",
    "Distortion of statistical measures: Outliers can greatly influence statistical measures such as the mean and standard deviation. Since these measures are sensitive to extreme values, outliers can distort the estimated parameters and provide a misleading representation of the data.\n",
    "\n",
    "Skewed analysis and modeling: Outliers can introduce bias in data analysis and modeling techniques. For instance, in regression analysis, outliers can exert undue influence on the estimated regression coefficients, leading to inaccurate predictions and model interpretation. By removing or addressing outliers, more accurate and reliable models can be built.\n",
    "\n",
    "Impact on data distributions: Outliers can affect the distributional assumptions of many statistical methods. If the data distribution is assumed to be normal or symmetric, outliers can violate these assumptions, potentially affecting the validity of subsequent analyses or hypothesis testing.\n",
    "\n",
    "Robustness of algorithms: Some algorithms used in data analysis and machine learning are sensitive to outliers. Outliers can skew the decision boundaries or clustering structures, leading to suboptimal results. Handling outliers can improve the robustness and performance of these algorithms.\n",
    "\n",
    "Data quality and integrity: Outliers can indicate potential errors in data collection or data entry processes. By identifying and handling outliers, data quality and integrity can be improved. Additionally, outliers can also indicate interesting phenomena or events worthy of further investigation.\n",
    "\n",
    "It is important to note that handling outliers should be done carefully and should depend on the specific context and objectives of the analysis. Outliers can sometimes contain valuable information or be genuine observations, so blindly removing them without a thorough understanding of the data and domain knowledge can lead to erroneous conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec7cfd-7755-4a9e-9578-f5befe34b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q7.\n",
    "\n",
    "Handling missing data is a common challenge in data analysis. Here are some techniques you can use to handle missing data:\n",
    "\n",
    "Deletion:\n",
    "\n",
    "Listwise deletion: Also known as complete case analysis, this involves removing entire rows with missing values. It is suitable when the missing data is randomly distributed and does not introduce bias.\n",
    "Pairwise deletion: In this approach, only the specific variables involved in a particular analysis are considered, and missing values in other variables are ignored. This allows maximum use of available data but can lead to inconsistent results if data are missing systematically.\n",
    "Imputation:\n",
    "\n",
    "Mean/median imputation: Replace missing values with the mean or median of the available data for that variable. This method is simple and preserves the overall distribution, but it does not account for any relationships between variables.\n",
    "Regression imputation: Predict missing values based on a regression model using other variables as predictors. This method considers relationships between variables but assumes a linear relationship.\n",
    "Multiple imputation: Generate multiple plausible imputations by modeling the missing values based on observed data. The analysis is then performed on each imputed dataset, and the results are combined to account for the uncertainty introduced by imputation.\n",
    "Indicator variable:\n",
    "\n",
    "Create an indicator variable that indicates whether a value is missing or not. This approach allows the missingness to be considered as a separate category in the analysis, capturing potential differences between missing and non-missing values.\n",
    "Domain-specific methods:\n",
    "\n",
    "Depending on the nature of the data and domain knowledge, there may be specialized techniques to handle missing data. For example, in time series data, interpolation or forward/backward filling may be appropriate.\n",
    "It is crucial to choose an appropriate technique based on the specific characteristics of your dataset, the underlying missing data mechanism, and the analysis objectives. Additionally, it is essential to evaluate the potential impact of missing data on the validity of the results and consider any assumptions made during the handling process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cb8f3-b7a0-4b3f-b8d8-692ba48ab505",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q8.\n",
    "Determining if missing data is missing at random (MAR) or if there is a pattern to the missing data can help in understanding the nature of the missingness and choosing appropriate strategies for handling it. Here are some strategies you can use to assess the missing data pattern:\n",
    "\n",
    "Visual exploration:\n",
    "\n",
    "Missing data matrix: Create a visual representation of the missingness pattern by plotting the missing values as a matrix. This matrix can help identify any visible patterns or clusters of missing values.\n",
    "Missing data heatmaps: Use heatmaps to visualize the correlation between missing values in different variables. If there are relationships or dependencies among missing values, they may indicate non-random missingness.\n",
    "Missingness tests:\n",
    "\n",
    "Little's MCAR test: Little's test is a statistical test that examines whether the missingness is completely random (MCAR) or not. The test compares the observed pattern of missing data with an expected pattern of missing data under the assumption of MCAR.\n",
    "Missing data pattern analysis: Analyze the patterns and relationships of missing values across variables. For example, if missingness is related to specific demographic variables, it could suggest non-random missingness.\n",
    "Imputation and analysis comparison:\n",
    "\n",
    "Perform complete case analysis: Conduct analyses on complete cases (rows without missing values) and compare the results with analyses including missing data. If the results differ significantly, it may indicate non-random missingness.\n",
    "Imputation sensitivity analysis: Apply different imputation methods and compare the results. If the results vary substantially based on the imputation method, it suggests that the missing data pattern can influence the analysis outcomes.\n",
    "Domain knowledge and expert input:\n",
    "\n",
    "Seek insights from domain experts: Consult with subject-matter experts who are familiar with the data and context. They might have insights into potential patterns or reasons for missingness that can guide your investigation.\n",
    "Remember that determining the missing data pattern is not always straightforward and can be challenging. It requires a combination of statistical analysis, visualization techniques, and subject-matter expertise. It is important to carefully interpret the results and consider multiple approaches to gain a comprehensive understanding of the missingness in your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211467d-6a77-4e36-9726-8b5cb5deea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q9.\n",
    "\n",
    "Dealing with imbalanced datasets in the context of medical diagnosis can be challenging, as the class of interest (e.g., the condition being diagnosed) is often the minority class. Here are some strategies you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "Use appropriate evaluation metrics:\n",
    "\n",
    "Accuracy alone is not a reliable metric in imbalanced datasets. Instead, consider metrics that provide a more comprehensive view of the model's performance. Some useful metrics include:\n",
    "Precision: Calculates the proportion of true positive predictions out of the total predicted positive instances. It focuses on the accuracy of positive predictions and is suitable when the cost of false positives is high.\n",
    "Recall (Sensitivity): Measures the proportion of true positive predictions out of the total actual positive instances. It focuses on identifying all positive instances and is suitable when the cost of false negatives is high.\n",
    "F1 score: Harmonic mean of precision and recall, providing a balanced metric that considers both precision and recall. It is useful when there is an uneven distribution between the two classes.\n",
    "Area Under the ROC Curve (AUC-ROC): Measures the model's ability to rank instances correctly, considering various classification thresholds.\n",
    "Resampling techniques:\n",
    "\n",
    "Oversampling: Increase the representation of the minority class by duplicating or creating synthetic samples from the existing minority samples. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can be employed.\n",
    "Undersampling: Decrease the representation of the majority class by randomly removing instances. Care should be taken to preserve the essential information and avoid significant loss.\n",
    "Combination (hybrid) methods: Combine oversampling and undersampling to generate a more balanced dataset. This can help mitigate the risk of overfitting and improve model performance.\n",
    "Adjust classification thresholds:\n",
    "\n",
    "By adjusting the classification threshold, you can emphasize precision or recall based on the specific requirements of your project. Setting a higher threshold can increase precision but decrease recall, and vice versa. It's a trade-off that should be determined considering the consequences of false positives and false negatives.\n",
    "Ensemble methods:\n",
    "\n",
    "Utilize ensemble learning techniques such as bagging, boosting, or stacking to combine multiple models and benefit from their collective performance. Ensemble methods can help improve the model's ability to capture the minority class.\n",
    "Collect more data:\n",
    "\n",
    "If feasible, consider acquiring additional data to balance the class distribution. This can help improve the model's performance by providing a more representative sample of the population.\n",
    "It's important to note that different strategies may work better for different datasets and algorithms. Experimentation and careful evaluation of the results are crucial in finding the most effective approach for your specific medical diagnosis project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694b17b-70fa-461c-9348-a6f57633cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q10.\n",
    "\n",
    "When dealing with an unbalanced dataset where the majority class dominates the data, you can employ various methods to balance the dataset and down-sample the majority class. Here are some commonly used techniques:\n",
    "\n",
    "Random under-sampling:\n",
    "\n",
    "Randomly select a subset of the majority class samples to match the number of samples in the minority class. This method can be quick and straightforward, but it may discard potentially valuable information present in the majority class.\n",
    "Cluster-based under-sampling:\n",
    "\n",
    "Use clustering algorithms to identify dense regions in the majority class. Then, within each cluster, randomly select samples to match the number of samples in the minority class. This approach aims to preserve the local structure of the majority class while reducing its overall size.\n",
    "Tomek links:\n",
    "\n",
    "Identify pairs of samples from different classes that are nearest neighbors of each other. Remove the majority class sample from each pair. This method focuses on the boundary samples that are most likely to cause misclassification.\n",
    "Edited nearest neighbors:\n",
    "\n",
    "Identify misclassified majority class samples by using a classifier. Remove the samples that are misclassified, effectively downsampling the majority class.\n",
    "NearMiss:\n",
    "\n",
    "NearMiss is a family of under-sampling techniques that select samples from the majority class based on their distances to the minority class. The goal is to retain informative samples that are close to the decision boundary.\n",
    "Down-sampling with imbalanced-learn library:\n",
    "\n",
    "The imbalanced-learn library provides various under-sampling techniques specifically designed for imbalanced datasets. It includes methods such as RandomUnderSampler, ClusterCentroids, and EditedNearestNeighbours.\n",
    "Remember that down-sampling the majority class should be done with caution, as it involves discarding data. This approach can result in loss of information and potential underrepresentation of the majority class, leading to biased results. It's important to carefully evaluate the impact of down-sampling on your model's performance and consider other techniques like over-sampling or hybrid methods if necessary.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
